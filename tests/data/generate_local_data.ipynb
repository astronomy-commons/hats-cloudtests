{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCAL unit test data\n",
    "\n",
    "There are two types of data used in unit tests in this repo: local and cloud. This notebook concerns itself only with the local versions of test data, so you can re-generate it.\n",
    "\n",
    "## Object catalog: small sky\n",
    "\n",
    "This is the same \"object catalog\" with 131 randomly generated radec values inside the order0-pixel11 healpix pixel that is used in hats and LSDB unit test suites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import hats_import.pipeline as runner\n",
    "from hats_import.catalog.arguments import ImportArguments\n",
    "from dask.distributed import Client\n",
    "from hats.io.file_io import remove_directory\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "client = Client(n_workers=1, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=\"small_sky_parts\",\n",
    "        output_path=\".\",\n",
    "        highest_healpix_order=1,\n",
    "        file_reader=\"csv\",\n",
    "        output_artifact_name=\"small_sky\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_order1\n",
    "\n",
    "This catalog has the same data points as other small sky catalogs, but is coerced to spreading these data points over partitions at order 1, instead of order 0.\n",
    "\n",
    "This means there are 4 leaf partition files, instead of just 1, and so can be useful for confirming reads/writes over multiple leaf partition files.\n",
    "\n",
    "NB: Setting `constant_healpix_order` coerces the import pipeline to create leaf partitions at order 1.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_order1\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=\"small_sky_parts\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        output_artifact_name=\"small_sky_order1\",\n",
    "        constant_healpix_order=1,\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    runner.pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!\n",
    "\n",
    "Everything else is raw files, and things that need to be manipulated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path.cleanup()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

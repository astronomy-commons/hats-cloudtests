{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLOUD unit test data\n",
    "\n",
    "There are two types of data used in unit tests in this repo: local and cloud. This notebook concerns itself only with the CLOUD versions of test data, so you can re-generate it.\n",
    "\n",
    "This also works to initialize data in a new cloud provider, instead of simply copying an existing data set.\n",
    "\n",
    "## Object catalog: small sky\n",
    "\n",
    "This is the same \"object catalog\" with 131 randomly generated radec values inside the order0-pixel11 healpix pixel that is used in HATS and LSDB unit test suites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from upath import UPath\n",
    "import shutil\n",
    "\n",
    "import hats\n",
    "from hats_import import ImportArguments, pipeline_with_client, CollectionArguments\n",
    "from dask.distributed import Client\n",
    "from hats.io.file_io import remove_directory\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "storage_options = {\n",
    "    \"account_key\": os.environ.get(\"ABFS_LINCCDATA_ACCOUNT_KEY\"),\n",
    "    \"account_name\": os.environ.get(\"ABFS_LINCCDATA_ACCOUNT_NAME\"),\n",
    "}\n",
    "storage_options\n",
    "\n",
    "\n",
    "output_path = UPath(\"../cloud/data\")\n",
    "\n",
    "client = Client(n_workers=1, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(output_path / \"small_sky\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=\"small_sky_parts\",\n",
    "        highest_healpix_order=1,\n",
    "        file_reader=\"csv\",\n",
    "        output_path=output_path,\n",
    "        output_artifact_name=\"small_sky\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_order1\n",
    "\n",
    "This catalog has the same data points as other small sky catalogs, but is coerced to spreading these data points over partitions at order 1, instead of order 0.\n",
    "\n",
    "This means there are 4 leaf partition files, instead of just 1, and so can be useful for confirming reads/writes over multiple leaf partition files.\n",
    "\n",
    "NB: Setting `constant_healpix_order` coerces the import pipeline to create leaf partitions at order 1.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(output_path / \"small_sky_order1\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = (\n",
    "        CollectionArguments(\n",
    "            output_path=output_path,\n",
    "            output_artifact_name=\"small_sky_order1\",\n",
    "            tmp_dir=pipeline_tmp,\n",
    "        )\n",
    "        .catalog(\n",
    "            input_path=\"small_sky_parts\",\n",
    "            file_reader=\"csv\",\n",
    "            constant_healpix_order=1,\n",
    "            output_artifact_name=\"small_sky_order1\",\n",
    "        )\n",
    "        .add_margin(\n",
    "            output_artifact_name=\"small_sky_order1_margin\",\n",
    "            margin_threshold=7200,\n",
    "            is_default=True,\n",
    "        )\n",
    "        .add_index(\n",
    "            indexing_column=\"id\",\n",
    "            output_artifact_name=\"small_sky_object_index\",\n",
    "        )\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_xmatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(output_path / \"small_sky_xmatch\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_file_list=[\"xmatch/xmatch_catalog_raw.csv\"],\n",
    "        file_reader=\"csv\",\n",
    "        constant_healpix_order=1,\n",
    "        output_path=output_path,\n",
    "        output_artifact_name=\"small_sky_xmatch\",\n",
    "        pixel_threshold=100,\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small_sky_npix_as_dir\n",
    "\n",
    "Copies small_sky but makes Npix a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npix_suffix = \"/\"\n",
    "\n",
    "sso = hats.read_hats(output_path / \"small_sky\")\n",
    "paths = [hats.io.paths.pixel_catalog_file(sso.catalog_base_dir, pixel) for pixel in sso.get_healpix_pixels()]\n",
    "\n",
    "out_catalog_name = \"small_sky_npix_as_dir\"\n",
    "out_catalog_path = output_path / out_catalog_name\n",
    "out_catalog_info = sso.catalog_info.copy_and_update(catalog_name=out_catalog_name, npix_suffix=npix_suffix)\n",
    "out_dirs = [\n",
    "    hats.io.paths.pixel_catalog_file(out_catalog_path, pixel, npix_suffix=npix_suffix)\n",
    "    for pixel in sso.get_healpix_pixels()\n",
    "]\n",
    "\n",
    "for path, out_dir in zip(paths, out_dirs):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # hats/lsdb will only care about `out_dir`. They will be agnostic to filenames, given `npix_suffix = \"/\"`.\n",
    "    shutil.copy(path, out_dir / \"part0.parquet\")\n",
    "hats.io.write_parquet_metadata(out_catalog_path)\n",
    "out_catalog_info.to_properties_file(out_catalog_path)\n",
    "sso.partition_info.write_to_file(hats.io.paths.get_partition_info_pointer(out_catalog_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path.cleanup()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "july",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
